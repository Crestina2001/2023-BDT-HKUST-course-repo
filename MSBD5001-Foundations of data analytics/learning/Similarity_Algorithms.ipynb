{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95aa0ac9",
   "metadata": {},
   "source": [
    "<p>Generally speaking, all the methods learnt in this lecture is pre-ML era methods</p>\n",
    "<p>More advanced methods include word embeddings and byte-pair encoding</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d1f8e",
   "metadata": {},
   "source": [
    "# 2.Set-based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89384ab3",
   "metadata": {},
   "source": [
    "## 2.1 TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff30f78",
   "metadata": {},
   "source": [
    "<p>Term Frequency (TF): The term frequency represents how often a term appears in a document. It represents how important a term is to one document.<\\p>\n",
    "<p>Inverse Document Frequency (IDF): The IDF factor balances out the importance given to terms that appear very frequently across many documents in the corpus. It can help decrease the significance of the words that frequently appear in all documents.<\\p>\n",
    "<p>TF/IDF = TF * IDF<\\p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6ee72f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469791</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538648</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.267104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469791</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and  document     first        is       one    second       the  \\\n",
       "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
       "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
       "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
       "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
       "\n",
       "      third      this  \n",
       "0  0.000000  0.384085  \n",
       "1  0.000000  0.281089  \n",
       "2  0.511849  0.267104  \n",
       "3  0.000000  0.384085  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample text data\n",
    "documents = [\n",
    "    'this is the first document',\n",
    "    'this document is the second document',\n",
    "    'and this is the third one',\n",
    "    'is this the first document'\n",
    "]\n",
    "\n",
    "# Create the TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to dense matrix and then to DataFrame for better visualization\n",
    "df = pd.DataFrame(tfidf_matrix.todense(), columns=feature_names)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38ed3e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469791</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538648</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.267104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469791</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and  document     first        is       one    second       the  \\\n",
       "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
       "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
       "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
       "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
       "\n",
       "      third      this  \n",
       "0  0.000000  0.384085  \n",
       "1  0.000000  0.281089  \n",
       "2  0.511849  0.267104  \n",
       "3  0.000000  0.384085  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Return a Counter object\n",
    "# input: 'this is the first document'\n",
    "# output: Counter({'this': 0.2, 'is': 0.2, 'the': 0.2, 'first': 0.2, 'document': 0.2})\n",
    "def compute_tf(text):\n",
    "    \"\"\"Compute term frequency for each term in a document\"\"\"\n",
    "    tf_text = Counter(text.split())\n",
    "    for term in tf_text:\n",
    "        tf_text[term] = tf_text[term] / float(len(text.split()))\n",
    "    return tf_text\n",
    "\n",
    "# Return a dictionary, storing the {word i : log((1 + N)/(1 + #appearance of word i)) + 1}\n",
    "def compute_idf(documents):\n",
    "    \"\"\"Compute inverse document frequency for each term in the corpus\"\"\"\n",
    "    idf_dict = {}\n",
    "    N = len(documents)\n",
    "    \n",
    "    for doc in documents:\n",
    "        for word in set(doc.split()):\n",
    "            idf_dict[word] = idf_dict.get(word, 0) + 1\n",
    "    \n",
    "    for word, val in idf_dict.items():\n",
    "        idf_dict[word] = math.log((1 + N) / (1 + val)) + 1  # sklearn's IDF formula\n",
    "        \n",
    "    return idf_dict\n",
    "\n",
    "def normalize_vector(vector):\n",
    "    \"\"\"Apply L2-normalization to a vector\"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def compute_tfidf(documents):\n",
    "    \"\"\"Compute TF-IDF score for each term in each document\"\"\"\n",
    "    tf_values = [compute_tf(doc) for doc in documents]\n",
    "    idf_values = compute_idf(documents)\n",
    "    \n",
    "    # Sort feature names\n",
    "    unique_words = sorted(list(idf_values.keys()))\n",
    "    \n",
    "    tfidf_values = []\n",
    "    for tf in tf_values:\n",
    "        tfidf = {}\n",
    "        for word, val in tf.items():\n",
    "            tfidf[word] = val * idf_values.get(word, 0)\n",
    "        tfidf_values.append(tfidf)\n",
    "    \n",
    "    # Convert to matrix form and apply L2-normalization\n",
    "    tfidf_matrix = np.zeros((len(documents), len(unique_words)))\n",
    "    for i, tfidf in enumerate(tfidf_values):\n",
    "        for j, word in enumerate(unique_words):\n",
    "            tfidf_matrix[i, j] = tfidf.get(word, 0)\n",
    "        tfidf_matrix[i, :] = normalize_vector(tfidf_matrix[i, :])\n",
    "    \n",
    "    return tfidf_matrix, unique_words\n",
    "\n",
    "# Sample text data\n",
    "documents = [\n",
    "    'this is the first document',\n",
    "    'this document is the second document',\n",
    "    'and this is the third one',\n",
    "    'is this the first document'\n",
    "]\n",
    "\n",
    "# Compute TF-IDF values\n",
    "tfidf_matrix, feature_names = compute_tfidf(documents)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "df2 = pd.DataFrame(tfidf_matrix, columns=feature_names)\n",
    "\n",
    "# Display the DataFrame\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fff89af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    and  document  first    is   one  second   the  third  this\n",
       "0  True      True   True  True  True    True  True   True  True\n",
       "1  True      True   True  True  True    True  True   True  True\n",
       "2  True      True   True  True  True    True  True   True  True\n",
       "3  True      True   True  True  True    True  True   True  True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.absolute(df2-df)<=1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c286b",
   "metadata": {},
   "source": [
    "## 2.2 Generalized Jacard Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1db84f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9722222222222223\n"
     ]
    }
   ],
   "source": [
    "def jaro_winkler(s1, s2):\n",
    "    # If the strings are equal\n",
    "    if s1 == s2:\n",
    "        return 1.0\n",
    "\n",
    "    # Length of two strings\n",
    "    len1, len2 = len(s1), len(s2)\n",
    "    max_dist = int(max(len1, len2) / 2) - 1\n",
    "\n",
    "\n",
    "    # identifies and records characters in s1 that have a corresponding match in s2 within a window\n",
    "    match1, match2 = [], []\n",
    "    count = 0\n",
    "    for i, l1 in enumerate(s1):\n",
    "        left, right = max(0, i - max_dist), min(i + max_dist + 1, len2)\n",
    "        if l1 in s2[left:right]:\n",
    "            count += 1\n",
    "            match1.append(l1)\n",
    "            match2.append(s2[s2.index(l1)])\n",
    "            # Replace a found character with * to ensure we don't match it again with another character from s1\n",
    "            s2 = s2[0:s2.index(l1)] + '*' + s2[s2.index(l1) + 1:]\n",
    "\n",
    "    # If no characters match\n",
    "    if count == 0:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "    # Compute transpositions\n",
    "    t = sum([match1[i] != match2[i] for i in range(len(match1))]) / 2.0\n",
    "\n",
    "\n",
    "    score = ((count / len1) + (count / len2) + ((count - t) / count)) / 3\n",
    "\n",
    "    # Adjust for Jaro-Winkler, giving a bonus to strings that have a common prefix\n",
    "    l = min(len([i for i in range(len(match1)) if s1[i] == s2[i]]), 4)\n",
    "    score += l * 0.1 * (1 - score)\n",
    "\n",
    "    return score\n",
    "\n",
    "def generalized_jaccard(set1, set2, threshold=0.5):\n",
    "    if not set1 or not set2:\n",
    "        return 0.0\n",
    "\n",
    "    # Convert to sets if they are lists\n",
    "    set1, set2 = set(set1), set(set2)\n",
    "\n",
    "    match_score = 0.0\n",
    "    match_count = 0\n",
    "\n",
    "    for element in set1:\n",
    "        max_score = 0\n",
    "        for item in set2:\n",
    "            score = jaro_winkler(element, item)\n",
    "            if score > threshold:\n",
    "                max_score = max(max_score, score)\n",
    "        \n",
    "        if max_score > 0:\n",
    "            match_score += max_score\n",
    "            match_count += 1\n",
    "\n",
    "    return float(match_score) / float(len(set1) + len(set2) - match_count)\n",
    "\n",
    "# Test\n",
    "print(generalized_jaccard(['apple', 'banana'], ['appel', 'banan'], threshold=0.8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d440dd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 0.16666666666666666\n",
      "Generalized Jaccard Similarity: 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# Jaccard Similarity\n",
    "def jaccard_similarity(str1, str2):\n",
    "    a = set(str1.split())\n",
    "    b = set(str2.split())\n",
    "    return len(a.intersection(b)) / float(len(a.union(b)))\n",
    "\n",
    "# Generalized Jaccard Similarity\n",
    "def generalized_jaccard_similarity(str1, str2, threshold=0.7):\n",
    "    tokens_a = str1.split()\n",
    "    tokens_b = str2.split()\n",
    "\n",
    "    # Create and fit a single CountVectorizer on both strings\n",
    "    vectorizer = CountVectorizer().fit(tokens_a + tokens_b)\n",
    "\n",
    "    # Transform each string using the shared vocabulary\n",
    "    vec_a = vectorizer.transform(tokens_a).toarray()\n",
    "    vec_b = vectorizer.transform(tokens_b).toarray()\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    sim_matrix = cosine_similarity(vec_a, vec_b)\n",
    "\n",
    "    # Thresholding\n",
    "    sim_matrix[sim_matrix < threshold] = 0\n",
    "\n",
    "    # Maximum-weight bipartite matching\n",
    "    row_ind, col_ind = linear_sum_assignment(-sim_matrix)\n",
    "    total_sim = sim_matrix[row_ind, col_ind].sum()\n",
    "\n",
    "    # Generalized Jaccard formula\n",
    "    return total_sim / (len(tokens_a) + len(tokens_b) - total_sim)\n",
    "\n",
    "# Test the functions\n",
    "str1 = \"Energy & Transportation\"\n",
    "str2 = \"Transportation, Energy, & Gas\"\n",
    "\n",
    "print(\"Jaccard Similarity:\", jaccard_similarity(str1, str2))\n",
    "print(\"Generalized Jaccard Similarity:\", generalized_jaccard_similarity(str1, str2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e289b5",
   "metadata": {},
   "source": [
    "## 2.3 Soft TF-IDF measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a397edad",
   "metadata": {},
   "source": [
    "<p>Firstly, we get a TF-IDF Matrix M. </p>\n",
    "<p>Soft TF-IDF is used to compute the similarity between document A and B: for every word i in A, find a most similar word j in B(there is a threshold, if no word in B has a similarity score over the threshold, then this term is 0), and add the following term to the overall score  $$TF-IDF(i) * TF-IDF(j) * similarity(i,j)$$</p>\n",
    "<p> Its physical meaning is the addition of: the importance of word i in document A * the importance of word j in document B * similarity score</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d3030d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7683311334449655\n"
     ]
    }
   ],
   "source": [
    "def compute_tf_idf_weight(term, document, corpus):\n",
    "    tf = document.count(term) / len(document)\n",
    "    idf = len(corpus) / sum([1 for doc in corpus if term in doc])\n",
    "    return tf * idf\n",
    "\n",
    "def soft_tf_idf(x, y, corpus, threshold):\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda s: s.split()).fit(corpus)\n",
    "    vocab = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # x_tf and y_tf are tf-idf matrices of vectors x and y\n",
    "    x_tf = tfidf_vectorizer.transform([x])\n",
    "    y_tf = tfidf_vectorizer.transform([y])\n",
    "\n",
    "\n",
    "    soft_score = 0\n",
    "\n",
    "    for word_x in x.split():\n",
    "        if word_x in vocab:\n",
    "            max_sim = 0\n",
    "            sim_word = \"\"\n",
    "            for word_y in y.split():\n",
    "                if word_y in vocab:\n",
    "                    sim = jaro_winkler(word_x, word_y)\n",
    "                    if sim > threshold and sim > max_sim:\n",
    "                        max_sim = sim\n",
    "                        sim_word = word_y\n",
    "            x_idf_index = tfidf_vectorizer.vocabulary_[word_x]\n",
    "            x_tf_value = x_tf[0, x_idf_index]\n",
    "            y_idf_index = tfidf_vectorizer.vocabulary_[sim_word] if sim_word in vocab else -1\n",
    "            y_tf_value = y_tf[0, y_idf_index] if y_idf_index != -1 else 0\n",
    "            soft_score += x_tf_value * y_tf_value * max_sim\n",
    "\n",
    "    return soft_score\n",
    "\n",
    "# Example\n",
    "corpus = [\"apple is good for health\", \n",
    "          \"Apple's headquarter is in California\", \n",
    "          \"Apple is good for helth\", \n",
    "          \"A17 is not as good as expected\"]\n",
    "x = \"apple is good for health\"\n",
    "y = \"Apple is good for helth\"\n",
    "print(soft_tf_idf(x, y, corpus, 0.75))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edbdbda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
